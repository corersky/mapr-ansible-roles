---
# tasks file for mapr-aws-bootstrap

  - name: create nodes for non-cluster roles
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      spot_price: '{{edge_node_price}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}'
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{edge_node_type}}'
      count: '{{edge_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        volume_size: 24
        delete_on_termination: true
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
        user: '{{ec2_keypair}}'
    register: external

  - name: create cluster nodes
    action:
      module: ec2
      region: '{{ec2_region}}'
      zone: '{{ec2_zone}}'
      spot_price: '{{cluster_node_price}}'
      key_name: '{{ec2_keypair}}'
      group: '{{ec2_security_group}}'
      image: '{{ec2_image}}' # This is a CentOS 6 image
      vpc_subnet_id: '{{vpc_subnet}}'
      assign_public_ip: true
      instance_type: '{{cluster_node_type}}'
      count: '{{cluster_node_count}}'
      wait: yes
      wait_timeout: '{{ec2_wait_timeout}}'
      volumes:
      - device_name: '{{root_device_path}}'
        # confusingly, the root device is referred to as /dev/sda in AWS console, but
        # the device in linux is /dev/xvde. If you want to change the size of the underlying
        # volume as we are here, the device_name above MUST be /dev/sda. Unsure whether this
        # changes with different AMIs. Additional volumes do not seem to have this problem.
        volume_size: 24
        delete_on_termination: true
      - device_name: /dev/xvdf
        volume_size: 128
        delete_on_termination: true
        ephemeral: ephemeral0
      - device_name: /dev/xvdg
        volume_size: 128
        delete_on_termination: true
        ephemeral: ephemeral1
      monitoring: yes
      instance_tags:
        Name: '{{ec2_name_tag}}'
        user: '{{ec2_keypair}}'
    register: ec2_cluster

  - name: write an inventory file containing the just-created cluster (small-production)
    template: src=cluster.hosts.frances.j2 dest='./cluster.hosts' mode=0644 backup=yes
    when: cluster_node_count == 6 

  - name: wait for the hosts to be available via ssh before moving on
    local_action: wait_for host='{{item.private_ip}}' port=22 delay=30 timeout=300 state=started
    with_items: ec2_cluster.instances
    when: ec2_cluster.instances is defined

  - name: add the new hosts to inventory
    add_host: name={{item.private_ip}} groups=instances ansible_ssh_user='{{ssh_user}}' public_ip='{{item.public_ip}}' private_ip='{{item.private_ip}}' public_dns_name='{{item.public_dns_name}}' private_dns_name='{{item.private_dns_name}}'
    with_items: ec2_cluster.instances
    when: ec2_cluster.instances is defined

  - name: save hosts file
    run_once: yes
    local_action: template src=hosts.j2 dest='./hosts_entry' mode=0644 backup=no
