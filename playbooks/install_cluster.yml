
--- 
# First, install the cluster - all nodes running Hadoop services.
- hosts: all
  gather_facts: no
  tasks:
    - name: get ec2 facts
      ec2_facts:
      register: ec2_facts

- hosts: all
  tasks:
    - group_by: key={{ansible_virtualization_type}}
    - group_by: key={{ansible_distribution}}

# Redhat derived distro specific prereqs
- hosts: CentOS;Redhat
  max_fail_percentage: 0
  sudo: yes
  roles:
    - mapr-redhat-prereqs

# Debian derived distro specific prereqs
- hosts: Debian;Ubuntu
  max_fail_percentage: 0
  sudo: yes
  roles:
    - mapr-debian-prereqs

- hosts: zookeepers
  max_fail_percentage: 0
  sudo: yes
  roles:
    - zookeeper

- hosts: cldb
  max_fail_percentage: 0
  sudo: yes
  roles:
    - mapr-cldb

- hosts: jobtracker
  max_fail_percentage: 0
  sudo: yes
  roles:
    - jobtracker

- hosts: tasktracker
  sudo: yes
  roles:
    - mapr-tasktracker

- hosts: webserver
  sudo: yes
  roles:
    - webserver

- hosts: nfs
  sudo: yes
  roles:
    - mapr-nfs

- hosts: fileserver
  sudo: yes
  roles:
    - mapr-fileserver

- hosts: zookeepers
  sudo: yes
  tasks:
    - name: start zookeeper on ZK nodes
      service: name=mapr-zookeeper state=started enabled=yes
      register: zk_started

    - name: wait for zookeeper to be listening
      wait_for: port=5181 delay=10 timeout=90

    - name: wait for a few seconds to let zookeepers initialize
      pause: seconds=15
      when: zk_started.changed

    - name: get zookeeper status
      command: service mapr-zookeeper qstatus
      register: zk_qstatus

- hosts: cldb
  sudo: yes
  tasks:
    - name: start warden on CLDBs first per doc.mapr.com
      service: name=mapr-warden state=started enabled=yes
      register: cldb_started

    - name: pause for a bit and wait for CLDBs to come up
      pause: seconds=15
      when: cldb_started.changed

- hosts: cluster
  sudo: yes
  tasks:
    - name: start the warden on all nodes
      service: name={{item}} state=started
      with_items:
        - mapr-warden
      register: warden_started

    - name: wait a bit before proceeding
      pause: seconds=10
      when: warden_started.changed

# Configure all cluster nodes with public key access
# This allows passwordless SSH between cluster nodes for root and mapr users.
- include: authorized_keys.yml

#
# Having installed and started the cluster, install optional ecosystem and client services.
# Some of the below will need to be uncommented, or be run manually.
#

# uncomment the following line to install mysql (hive metastore and metrics)
- include: install_mysql.yml

# uncomment the following line to install spark (spark, shark, spark-master)
#- include: install_spark.yml

# uncomment the following line to install hive
#- include: install_hive.yml

# uncomment the following line to install metrics
- include: install_metrics.yml


# For convenience, print the URL for the MCS.

- hosts: webserver
  sudo: yes
  gather_facts: no
  tasks:
    - name: get ec2 facts
      ec2_facts:
      register: ec2_facts

    - name: debug ec2_facts
      debug: var=ec2_facts

    - name: print webserver URLs
      debug: msg="webserver = https://{{ec2_facts.ansible_facts.ansible_ec2_public_hostname}}:8443"
      when: '"ansible_ec2_public_hostname" in ec2_facts.ansible_facts.keys()'

# if this is a virtualbox node, print out the IP for eth1, which is a host interface.
- hosts: virtualbox
  tasks:
    - name: print webserver URLs
      debug: msg="webserver=https://{{ansible_eth1.ipv4.address}}:8443"
      when: "ec2_facts is not defined"